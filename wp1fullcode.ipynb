{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wp1fullcode",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksaxena1998/packages/blob/master/wp1fullcode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6DbE9K3akqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "ddf858aa-bdab-4e0a-8506-29468e7fa3b7"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "df=pd.read_csv(\"ner_dataset.csv\",encoding='latin1')\n",
        "df = df.fillna(method='ffill')\n",
        "df\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_df=df['Word']\n",
        "y_df=df['Tag']\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, random_state=0)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
        "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
        "len(vect.get_feature_names())\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "predictions = model.predict(vect.transform(X_test))\n",
        "\n",
        "print('AUC: ', multiclass_roc_auc_score(y_test, predictions))\n",
        "\n",
        "import pickle\n",
        "\n",
        "filename = 'Log_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "#Importing MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#Initializing the MLPClassifier\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=100,activation = 'relu',solver='adam',random_state=1)\n",
        "\n",
        "def accuracy(confusion_matrix):\n",
        "   diagonal_sum = confusion_matrix.trace()\n",
        "   sum_of_all_elements = confusion_matrix.sum()\n",
        "   return diagonal_sum / sum_of_all_elements\n",
        "\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "#model = LogisticRegression()\n",
        "classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "predictions = classifier.predict(vect.transform(X_test))\n",
        "\n",
        "print('AUC: ', multiclass_roc_auc_score(y_test, predictions))\n",
        "\n",
        "#Fitting the training data to the network\n",
        "#classifier.fit(X_train, Y_train)\n",
        "#Predicting y for X_val\n",
        "#y_pred = classifier.predict(X_val)\n",
        "\n",
        "import pickle\n",
        "\n",
        "filename2 = 'mlp_model.sav'\n",
        "pickle.dump(classifier, open(filename2, 'wb'))\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.6375989551946686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC:  0.6776295507144383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHJpfUc_a4fQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46b82719-160e-49f6-c5fd-29bfbf71b6f2"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import warnings \n",
        "\n",
        "warnings.filterwarnings(action = 'ignore') \n",
        "\n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "\n",
        "sample = open(\"ncorpus.txt\", \"r\") \n",
        "s = sample.read() \n",
        "\n",
        "# Replaces escape character with space \n",
        "f = s.replace(\"\\n\", \" \") \n",
        "\n",
        "data = [] \n",
        "\n",
        "# iterate through each sentence in the file \n",
        "for i in sent_tokenize(f): \n",
        "\ttemp = [] \n",
        "\t\n",
        "\t# tokenize the sentence into words \n",
        "\tfor j in word_tokenize(i): \n",
        "\t\ttemp.append(j.lower()) \n",
        "\n",
        "\tdata.append(temp) \n",
        "\n",
        "# Create CBOW model \n",
        "mod = gensim.models.Word2Vec(data,min_count = 1, size = 100, window = 5) \n",
        "\n",
        "# Print results \n",
        "print(\"Cosine similarity between 'london' \" + \"and 'nation' - CBOW : \", mod.similarity('london', 'nation')) \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'london' and 'nation' - CBOW :  0.2733232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZGGOtokifzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod.save('cbow1.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jExRSPSlitg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modell= Word2Vec.load(\"cbow1.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82nlRO4ejP66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5dc7fd4-2c72-49ff-c3b5-34110fda9840"
      },
      "source": [
        "modell.similarity('uk', 'america')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7843357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIz3WGoQjXwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49936d08-4ccf-47c7-99fc-d7671e0ac5e7"
      },
      "source": [
        "phrase=input(\"Enter a sentence : \")\n",
        "l1=[]\n",
        "l2=[]\n",
        "l3=[]\n",
        "entity=phrase.split(' ')\n",
        "for d in entity:  \n",
        "    for i in data:\n",
        "          for j in i:\n",
        "                l1.append(d)\n",
        "                l2.append(j)\n",
        "                l3.append(modell.similarity(d, j))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a sentence : british are people of britain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr34AOsPkYWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "\n",
        "loaded_model2 = joblib.load('Log_model.sav')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQoi093tjfH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as numpy\n",
        "\n",
        "posq2=numpy.argmax(l3)\n",
        "topred=l2[posq2]\n",
        "#print(loaded_model.predict(vect.transform([topred])))\n",
        "arr=loaded_model2.predict(vect.transform([topred]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "901EZQbIkPst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7604c458-e68a-4593-d2e5-c8e2f21ed638"
      },
      "source": [
        "# import JSONEncoder class from json\n",
        "from json.encoder import JSONEncoder\n",
        "final_entity = { \"cat\": [arr[0]]}\n",
        "# directly called encode method of JSON\n",
        "JSONEncoder().encode(final_entity)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"cat\": [\"B-gpe\"]}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd5XOxXrkrC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}